\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{graphicx, float, subfigure, blindtext}
\usepackage{textcomp}
\usepackage{xcolor}
%\usepackage{subfigure}

\hypersetup{
    colorlinks=true,
    linkcolor=red}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\graphicspath{{./fig/}}
\begin{document}

\title{Visual Explanations for Deep Neural Nets}

\author{\IEEEauthorblockN{Siddhant Midha}
\IEEEauthorblockA{\textit{Sophomore, Electrical Engg.} \\
\textit{IIT Bombay}\\
\href{mailto:siddhantm@iitb.ac.in}{\texttt{siddhantm@iitb.ac.in}}}
\and
\IEEEauthorblockN{Prateek Garg}
\IEEEauthorblockA{\textit{Sophomore, Electrical Engg.} \\
\textit{IIT Bombay}\\
\href{mailto:prateekgarg@ee.iitb.ac.in}{\texttt{prateekgarg@ee.iitb.ac.in}}}}

\maketitle

\begin{abstract}
With the evolution of Machine Learning and Deep Learning, we have come up with more and more complex networks which perform very well on various tasks. Through this development, one caveat remains. Increasing complexity of the network leads to decreasing interpretability. In this report, we review few such methods and implement them. 
\end{abstract}

\begin{IEEEkeywords}
Saliency, Occlusion Sensitivity, Class Activation Maps 
\end{IEEEkeywords}

\section{Introduction}
There are two broad areas where an interpretation of the workings of neural networks proves useful - in the researcher's understanding, and as a proof of working to the stakeholders. We survey methods that facilitate the visualisation of the network, namely \textbf{Saliency Maps}, \textbf{Occlusion Sensitivity}, \textbf{Class Activation Maps} and \textbf{Deconvolution}. For some more insight, we also plot the learned \textbf{feature maps} at different locations in the network. The code can be found \href{https://github.com/prateekgargX/Visualizing-Neural-Networks}{here}.
\section{Methods}
\input{Saliency-Maps.tex}
\input{Occlusion-Senstivity-Map.tex}
\input{Class-Activation-Maps.tex}

\section{Results}
\input{SM.tex}
\input{OSM.tex}
\input{CAM.tex}
\section*{Acknowledgment}

We thank the \texttt{Analytics Club, IITB} for giving us the opportunity to work on this project, and we are grateful to our mentor Saikiran for his help.
\newpage
\begin{thebibliography}{00}
\bibitem{saliency} \href{https://arxiv.org/pdf/1312.6034.pdf}{Karen Simonyan, Andrea Vedaldi and Andrew Zisserman, ''Deep Inside Convolutional Networks: Visualising
Image Classification Models and Saliency Maps'' arXiv:1312.6034v2 [cs.CV] 19 Apr 2014}
\bibitem{occlusion} \href{https://arxiv.org/pdf/1311.2901.pdf}{Matthew D. Zeiler and Rob Fergus, ''Visualizing and Understanding Convolutional Networks'' arXiv:1311.2901v3 [cs.CV] 28 Nov 2013}
\bibitem{CAM} \href{http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf}{Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba ''Learning Deep Features for Discriminative Localization'' Computer Science and Artificial Intelligence Laboratory, MIT}
\bibitem{ResNet} \href{https://arxiv.org/pdf/1512.03385.pdf}{Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 'Deep Residual Learning for Image Recognition' arXiv:1512.03385 [cs.CV]}
\bibitem{Inception} \href{https://arxiv.org/pdf/1409.4842.pdf}{Wei Liu Et. al, Going deeper with convolutions arXiv:1409.4842v1 [cs.CV]}
\bibitem{SqueezeNet} \href{https://arxiv.org/pdf/1602.07360.pdf}{N. Iandola Et. al,'SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size'	arXiv:1602.07360 [cs.CV]}
\bibitem{DensNet} \href{https://arxiv.org/pdf/1608.06993.pdf}{Huang Et. al,"Densely Connected Convolutional Networks" arXiv:1608.06993 [cs.CV]}
\bibitem{dogvwolf} \href{https://www.kaggle.com/harishvutukuri/dogs-vs-wolves}{Kaggle Dogs vs Wolves Dataset}
\bibitem{ILSVRC} \href{https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description}{ImageNet Large Scale Visual Recognition Challenge (ILSVRC)}
\end{thebibliography}
\end{document}
